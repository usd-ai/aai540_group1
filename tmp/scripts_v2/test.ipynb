{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Feature Data for Full Model Training\n",
                "\n",
                "**Goal:** Verify all 20 engineered features are available and prepare for training with the complete feature set.\n",
                "\n",
                "## What We're Testing:\n",
                "1. Load Parquet features from S3\n",
                "2. Verify all 20 feature columns exist\n",
                "3. Check data quality\n",
                "4. Prepare parameters for pipeline execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "import pandas as pd\n",
                "import sagemaker\n",
                "\n",
                "# Initialize SageMaker session\n",
                "sess = sagemaker.Session()\n",
                "bucket = sess.default_bucket()\n",
                "region = sess.boto_region_name\n",
                "\n",
                "print(f\"Region: {region}\")\n",
                "print(f\"Default Bucket: {bucket}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Training Features from S3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Public bucket where features are stored\n",
                "PUBLIC_BUCKET = 'sagemaker-us-east-1-425709451100'\n",
                "FEATURES_PREFIX = 'aai540-group1/features'\n",
                "\n",
                "# Load training features\n",
                "train_path = f\"s3://{PUBLIC_BUCKET}/{FEATURES_PREFIX}/train_features.parquet\"\n",
                "print(f\"Loading: {train_path}\")\n",
                "\n",
                "df_train = pd.read_parquet(train_path)\n",
                "\n",
                "print(f\"\\nLoaded {len(df_train):,} rows √ó {df_train.shape[1]} columns\")\n",
                "print(f\"\\nColumns: {list(df_train.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verify All 20 Feature Columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define all 20 engineered features (including target-encoded rates)\n",
                "ALL_FEATURES = [\n",
                "    # Temporal (9)\n",
                "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE',\n",
                "    'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND',\n",
                "    \n",
                "    # Distance (4)\n",
                "    'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET',\n",
                "    \n",
                "    # Target-encoded (4) - THESE WERE EXCLUDED IN ENGINEERED BASELINE\n",
                "    'AIRLINE_DELAY_RATE', 'ORIGIN_DELAY_RATE', 'DEST_DELAY_RATE', 'ROUTE_DELAY_RATE',\n",
                "    \n",
                "    # Volume (3)\n",
                "    'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS'\n",
                "]\n",
                "\n",
                "TARGET = 'DELAYED'\n",
                "\n",
                "print(f\"Expected features: {len(ALL_FEATURES)}\")\n",
                "print(f\"\\nChecking for all features...\")\n",
                "\n",
                "missing_features = [f for f in ALL_FEATURES if f not in df_train.columns]\n",
                "if missing_features:\n",
                "    print(f\"‚ùå Missing features: {missing_features}\")\n",
                "else:\n",
                "    print(f\"‚úÖ All {len(ALL_FEATURES)} features are present!\")\n",
                "\n",
                "# Check target column\n",
                "if TARGET in df_train.columns:\n",
                "    print(f\"‚úÖ Target column '{TARGET}' found\")\n",
                "else:\n",
                "    print(f\"‚ùå Target column '{TARGET}' missing\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Inspect Data Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select only the features and target we need\n",
                "df_model = df_train[ALL_FEATURES + [TARGET]].copy()\n",
                "\n",
                "print(f\"Model dataset: {df_model.shape[0]:,} rows √ó {df_model.shape[1]} columns\")\n",
                "print(f\"\\nData types:\")\n",
                "print(df_model.dtypes)\n",
                "\n",
                "print(f\"\\nMissing values:\")\n",
                "missing = df_model.isnull().sum()\n",
                "if missing.sum() > 0:\n",
                "    print(missing[missing > 0])\n",
                "else:\n",
                "    print(\"No missing values ‚úÖ\")\n",
                "\n",
                "print(f\"\\nTarget distribution:\")\n",
                "delay_rate = df_model[TARGET].mean() * 100\n",
                "print(f\"Delayed: {(df_model[TARGET]==1).sum():,} ({delay_rate:.2f}%)\")\n",
                "print(f\"On-time: {(df_model[TARGET]==0).sum():,} ({100-delay_rate:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sample Data Preview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"First 5 rows (all 20 features + target):\")\n",
                "df_model.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Compare with Engineered Baseline (16 Features)\n",
                "\n",
                "The engineered baseline excluded these 4 target-encoded features:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Features that were EXCLUDED in engineered baseline\n",
                "EXCLUDED_IN_BASELINE = [\n",
                "    'AIRLINE_DELAY_RATE', \n",
                "    'ORIGIN_DELAY_RATE', \n",
                "    'DEST_DELAY_RATE', \n",
                "    'ROUTE_DELAY_RATE'\n",
                "]\n",
                "\n",
                "print(\"Target-encoded features (excluded in baseline, included now):\")\n",
                "print(df_model[EXCLUDED_IN_BASELINE].describe())\n",
                "\n",
                "print(\"\\nüìä These 4 features capture historical delay patterns:\")\n",
                "print(\"  - AIRLINE_DELAY_RATE: Historical delay rate for this airline\")\n",
                "print(\"  - ORIGIN_DELAY_RATE: Historical delay rate at origin airport\")\n",
                "print(\"  - DEST_DELAY_RATE: Historical delay rate at destination airport\")\n",
                "print(\"  - ROUTE_DELAY_RATE: Historical delay rate for this specific route\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Pipeline Parameters for Full Model Training\n",
                "\n",
                "Based on `02_engineered_baseline.ipynb` hyperparameters:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters from engineered baseline\n",
                "HYPERPARAMETERS = {\n",
                "    'Objective': 'binary:logistic',\n",
                "    'EvalMetric': 'auc',\n",
                "    'MaxDepth': 8,\n",
                "    'Eta': 0.05,\n",
                "    'NumRound': 1000,\n",
                "    'ScalePosWeight': 4.58,\n",
                "    'Subsample': 0.8,\n",
                "    'ColsampleByTree': 0.8,\n",
                "    'MinChildWeight': 1\n",
                "}\n",
                "\n",
                "# Data paths (Parquet format)\n",
                "DATA_PATHS = {\n",
                "    'TrainingDataUrl': f's3://{PUBLIC_BUCKET}/{FEATURES_PREFIX}/train/',\n",
                "    'ValidationDataUrl': f's3://{PUBLIC_BUCKET}/{FEATURES_PREFIX}/validation/',\n",
                "    'InputContentType': 'application/x-parquet'\n",
                "}\n",
                "\n",
                "print(\"üöÄ Pipeline Parameters for Full Model (20 features):\")\n",
                "print(\"\\nHyperparameters:\")\n",
                "for k, v in HYPERPARAMETERS.items():\n",
                "    print(f\"  --{k} {v}\")\n",
                "\n",
                "print(\"\\nData Configuration:\")\n",
                "for k, v in DATA_PATHS.items():\n",
                "    print(f\"  --{k} {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Command to Run Pipeline\n",
                "\n",
                "**Note:** The pipeline currently expects CSV format. You'll need to either:\n",
                "1. Use the CSV files prepared in `02_prepare_sagemaker_data.ipynb`, OR\n",
                "2. Prepare Parquet files with only the 20 features + target (no metadata columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if CSV training data exists\n",
                "s3_client = boto3.client('s3')\n",
                "\n",
                "csv_paths = [\n",
                "    f'{bucket}/aai540-group1/training/engineered-baseline/train/train.csv',\n",
                "    f'{bucket}/aai540-group1/training/engineered-baseline/val/val.csv'\n",
                "]\n",
                "\n",
                "print(\"Checking for prepared CSV training data...\")\n",
                "for path in csv_paths:\n",
                "    key = path.split('/', 1)[1]\n",
                "    try:\n",
                "        s3_client.head_object(Bucket=bucket, Key=key)\n",
                "        print(f\"  ‚úÖ Found: s3://{path}\")\n",
                "    except:\n",
                "        print(f\"  ‚ùå Missing: s3://{path}\")\n",
                "        print(f\"     Run: notebooks/02_feature_engineering/02_prepare_sagemaker_data.ipynb\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **If CSV data exists:** Run the pipeline with the command below\n",
                "2. **If CSV data missing:** Run `02_prepare_sagemaker_data.ipynb` first to create CSV files\n",
                "\n",
                "### Command to Run Pipeline (from terminal):\n",
                "\n",
                "```bash\n",
                "cd /Users/arr/USD/AAI-540/aai540_group1/tmp/scripts_v2\n",
                "\n",
                "python run_experiment_v2.py \\\n",
                "    --TrainingDataUrl s3://YOUR-BUCKET/aai540-group1/training/engineered-baseline/train/ \\\n",
                "    --ValidationDataUrl s3://YOUR-BUCKET/aai540-group1/training/engineered-baseline/val/ \\\n",
                "    --InputContentType text/csv \\\n",
                "    --MaxDepth 8 \\\n",
                "    --Eta 0.05 \\\n",
                "    --NumRound 1000 \\\n",
                "    --ScalePosWeight 4.58 \\\n",
                "    --display-name \"full-engineered-20-features\"\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}