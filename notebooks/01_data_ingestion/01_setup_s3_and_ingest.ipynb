{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a56fad",
   "metadata": {},
   "source": [
    "# S3 Datalake Setup and Data Ingestion\n",
    "\n",
    "**AAI-540 Group 1 - Flight Delay Prediction Project**\n",
    "\n",
    "## Objective\n",
    "This notebook performs the initial data ingestion for our MLOps project:\n",
    "1. Download the 2015 Flight Delays dataset from Kaggle\n",
    "2. Verify data integrity and file sizes\n",
    "3. Set up S3 bucket structure following MLOps best practices\n",
    "4. Upload raw data files to S3 datalake\n",
    "\n",
    "## Dataset\n",
    "- **Name:** 2015 Flight Delays and Cancellations\n",
    "- **Source:** U.S. DOT via Kaggle\n",
    "- **Files:** flights.csv (~5.8M rows), airlines.csv, airports.csv\n",
    "- **Size:** ~575MB compressed, ~2GB raw\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce7f82",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Import required libraries and load project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7938ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✓ Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install project dependencies from requirements.txt\n",
    "%pip install -q -r ../../requirements.txt\n",
    "\n",
    "print(\"✓ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683151d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AAI-540 Group 1 Project Configuration\n",
      "============================================================\n",
      "Region: us-east-1\n",
      "S3 Bucket: sagemaker-us-east-1-786869526001\n",
      "Project Prefix: aai540-group1/\n",
      "Athena Database: aai540_group1_db\n",
      "\n",
      "S3 Base URI: s3://sagemaker-us-east-1-786869526001/aai540-group1/\n",
      "============================================================\n",
      "\n",
      "Notebook executed at: 2026-01-23 07:48:41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project configuration\n",
    "from config import settings\n",
    "\n",
    "# Display configuration\n",
    "settings.print_config()\n",
    "\n",
    "print(f\"\\nNotebook executed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0fea1",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from Kaggle\n",
    "\n",
    "Using `kagglehub` to download the 2015 Flight Delays dataset.\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle API credentials configured (`~/.kaggle/kaggle.json`)\n",
    "- Or Kaggle authentication via environment variables\n",
    "\n",
    "The dataset will be downloaded to a local cache managed by kagglehub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334ba8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset: usdot/flight-delays\n",
      "This may take several minutes (~575MB compressed)...\n",
      "\n",
      "Downloading to /home/sagemaker-user/.cache/kagglehub/datasets/usdot/flight-delays/1.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191M/191M [00:01<00:00, 110MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset downloaded successfully!\n",
      "Path to dataset files: /home/sagemaker-user/.cache/kagglehub/datasets/usdot/flight-delays/versions/1\n",
      "Stored 'dataset_path' (str)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version of the flight delays dataset\n",
    "print(f\"Downloading dataset: {settings.DATASET_KAGGLE_ID}\")\n",
    "print(\"This may take several minutes (~575MB compressed)...\\n\")\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(settings.DATASET_KAGGLE_ID)\n",
    "\n",
    "print(f\"\\n✓ Dataset downloaded successfully!\")\n",
    "print(f\"Path to dataset files: {dataset_path}\")\n",
    "\n",
    "# Store path for later use\n",
    "%store dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff0b4c",
   "metadata": {},
   "source": [
    "## 3. Verify Downloaded Files\n",
    "\n",
    "Check that all expected data files are present and examine their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65e4d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files found:\n",
      "======================================================================\n",
      "✓ flights.csv          -   564.96 MB\n",
      "✓ airlines.csv         -     0.00 MB\n",
      "✓ airports.csv         -     0.02 MB\n",
      "======================================================================\n",
      "\n",
      "Total files found: 3 / 3\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path(dataset_path)\n",
    "\n",
    "print(\"Dataset files found:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "file_info = []\n",
    "for expected_file in settings.DATA_FILES.values():\n",
    "    file_path = dataset_dir / expected_file\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        file_info.append({\n",
    "            'File': expected_file,\n",
    "            'Size (MB)': f'{size_mb:.2f}',\n",
    "            'Status': '✓'\n",
    "        })\n",
    "        print(f\"✓ {expected_file:20s} - {size_mb:8.2f} MB\")\n",
    "    else:\n",
    "        file_info.append({\n",
    "            'File': expected_file,\n",
    "            'Size (MB)': 'N/A',\n",
    "            'Status': '✗ MISSING'\n",
    "        })\n",
    "        print(f\"✗ {expected_file:20s} - MISSING\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create DataFrame for summary\n",
    "df_files = pd.DataFrame(file_info)\n",
    "print(f\"\\nTotal files found: {df_files[df_files['Status'] == '✓'].shape[0]} / {len(settings.DATA_FILES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bcfc4",
   "metadata": {},
   "source": [
    "### Quick Data Preview\n",
    "\n",
    "Load a sample of each file to verify data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b1461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIGHTS.CSV Preview:\n",
      "----------------------------------------------------------------------\n",
      "Columns: 31\n",
      "   YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  \\\n",
      "0  2015      1    1            4      AS             98      N407AS   \n",
      "1  2015      1    1            4      AA           2336      N3KUAA   \n",
      "2  2015      1    1            4      US            840      N171US   \n",
      "3  2015      1    1            4      AA            258      N3HYAA   \n",
      "4  2015      1    1            4      AS            135      N527AS   \n",
      "\n",
      "  ORIGIN_AIRPORT DESTINATION_AIRPORT  SCHEDULED_DEPARTURE  ...  ARRIVAL_TIME  \\\n",
      "0            ANC                 SEA                    5  ...           408   \n",
      "1            LAX                 PBI                   10  ...           741   \n",
      "2            SFO                 CLT                   20  ...           811   \n",
      "3            LAX                 MIA                   20  ...           756   \n",
      "4            SEA                 ANC                   25  ...           259   \n",
      "\n",
      "   ARRIVAL_DELAY  DIVERTED  CANCELLED  CANCELLATION_REASON  AIR_SYSTEM_DELAY  \\\n",
      "0            -22         0          0                  NaN               NaN   \n",
      "1             -9         0          0                  NaN               NaN   \n",
      "2              5         0          0                  NaN               NaN   \n",
      "3             -9         0          0                  NaN               NaN   \n",
      "4            -21         0          0                  NaN               NaN   \n",
      "\n",
      "   SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
      "0             NaN            NaN                  NaN            NaN  \n",
      "1             NaN            NaN                  NaN            NaN  \n",
      "2             NaN            NaN                  NaN            NaN  \n",
      "3             NaN            NaN                  NaN            NaN  \n",
      "4             NaN            NaN                  NaN            NaN  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "AIRLINES.CSV Preview:\n",
      "----------------------------------------------------------------------\n",
      "Total airlines: 14\n",
      "  IATA_CODE                 AIRLINE\n",
      "0        UA   United Air Lines Inc.\n",
      "1        AA  American Airlines Inc.\n",
      "2        US         US Airways Inc.\n",
      "3        F9  Frontier Airlines Inc.\n",
      "4        B6         JetBlue Airways\n",
      "\n",
      "======================================================================\n",
      "\n",
      "AIRPORTS.CSV Preview:\n",
      "----------------------------------------------------------------------\n",
      "Total airports: 322\n",
      "  IATA_CODE                              AIRPORT         CITY STATE COUNTRY  \\\n",
      "0       ABE  Lehigh Valley International Airport    Allentown    PA     USA   \n",
      "1       ABI             Abilene Regional Airport      Abilene    TX     USA   \n",
      "2       ABQ    Albuquerque International Sunport  Albuquerque    NM     USA   \n",
      "3       ABR            Aberdeen Regional Airport     Aberdeen    SD     USA   \n",
      "4       ABY   Southwest Georgia Regional Airport       Albany    GA     USA   \n",
      "\n",
      "   LATITUDE  LONGITUDE  \n",
      "0  40.65236  -75.44040  \n",
      "1  32.41132  -99.68190  \n",
      "2  35.04022 -106.60919  \n",
      "3  45.44906  -98.42183  \n",
      "4  31.53552  -84.19447  \n"
     ]
    }
   ],
   "source": [
    "# Preview flights.csv (large file - just first 5 rows)\n",
    "flights_path = dataset_dir / settings.DATA_FILES['flights']\n",
    "print(\"FLIGHTS.CSV Preview:\")\n",
    "print(\"-\" * 70)\n",
    "df_flights_preview = pd.read_csv(flights_path, nrows=5)\n",
    "print(f\"Columns: {df_flights_preview.shape[1]}\")\n",
    "print(df_flights_preview.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Preview airlines.csv (small file)\n",
    "airlines_path = dataset_dir / settings.DATA_FILES['airlines']\n",
    "print(\"AIRLINES.CSV Preview:\")\n",
    "print(\"-\" * 70)\n",
    "df_airlines = pd.read_csv(airlines_path)\n",
    "print(f\"Total airlines: {df_airlines.shape[0]}\")\n",
    "print(df_airlines.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Preview airports.csv\n",
    "airports_path = dataset_dir / settings.DATA_FILES['airports']\n",
    "print(\"AIRPORTS.CSV Preview:\")\n",
    "print(\"-\" * 70)\n",
    "df_airports = pd.read_csv(airports_path)\n",
    "print(f\"Total airports: {df_airports.shape[0]}\")\n",
    "print(df_airports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22c447",
   "metadata": {},
   "source": [
    "## 4. Setup S3 Bucket Structure\n",
    "\n",
    "Initialize the S3 client and verify bucket access. The bucket structure follows MLOps best practices with separate directories for raw data, processed data, features, training, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9a6dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Bucket: sagemaker-us-east-1-786869526001\n",
      "Region: us-east-1\n",
      "\n",
      "S3 Structure:\n",
      "======================================================================\n",
      "  raw_data            : s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/\n",
      "  processed_data      : s3://sagemaker-us-east-1-786869526001/aai540-group1/data/processed/\n",
      "  parquet_data        : s3://sagemaker-us-east-1-786869526001/aai540-group1/data/parquet/\n",
      "  features            : s3://sagemaker-us-east-1-786869526001/aai540-group1/features/\n",
      "  athena_staging      : s3://sagemaker-us-east-1-786869526001/aai540-group1/athena/staging/\n",
      "  training_input      : s3://sagemaker-us-east-1-786869526001/aai540-group1/training/input/\n",
      "  training_output     : s3://sagemaker-us-east-1-786869526001/aai540-group1/training/output/\n",
      "  evaluation          : s3://sagemaker-us-east-1-786869526001/aai540-group1/evaluation/\n",
      "  batch_inference     : s3://sagemaker-us-east-1-786869526001/aai540-group1/inference/batch/\n",
      "  monitoring          : s3://sagemaker-us-east-1-786869526001/aai540-group1/monitoring/\n",
      "======================================================================\n",
      "\n",
      "✓ Bucket 'sagemaker-us-east-1-786869526001' is accessible\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Get bucket name from settings\n",
    "bucket_name = settings.DEFAULT_BUCKET\n",
    "\n",
    "print(f\"S3 Bucket: {bucket_name}\")\n",
    "print(f\"Region: {settings.REGION}\")\n",
    "print(f\"\\nS3 Structure:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for key, path in settings.S3_PATHS.items():\n",
    "    print(f\"  {key:20s}: {path}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify bucket exists and is accessible\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket_name)\n",
    "    print(f\"\\n✓ Bucket '{bucket_name}' is accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error accessing bucket: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aebb4f",
   "metadata": {},
   "source": [
    "## 5. Upload Raw Data to S3\n",
    "\n",
    "Upload all three CSV files to the S3 raw data location. We'll use boto3's upload_file with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0552b3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files to: s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/\n",
      "======================================================================\n",
      "\n",
      "Uploading: flights.csv\n",
      "  Size: 564.96 MB\n",
      "  S3 Key: aai540-group1/data/raw/flights.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flights.csv:  99%|█████████▉| 588M/592M [00:01<00:00, 478MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Upload complete\n",
      "\n",
      "Uploading: airlines.csv\n",
      "  Size: 0.00 MB\n",
      "  S3 Key: aai540-group1/data/raw/airlines.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flights.csv: 100%|██████████| 592M/592M [00:01<00:00, 330MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Upload complete\n",
      "\n",
      "Uploading: airports.csv\n",
      "  Size: 0.02 MB\n",
      "  S3 Key: aai540-group1/data/raw/airports.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "airlines.csv: 100%|██████████| 359/359 [00:00<00:00, 11.3kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Upload complete\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Upload Summary:\n",
      "        File Size (MB)                                                                    S3 URI    Status\n",
      " flights.csv    564.96  s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/flights.csv ✓ Success\n",
      "airlines.csv      0.00 s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/airlines.csv ✓ Success\n",
      "airports.csv      0.02 s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/airports.csv ✓ Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class S3ProgressCallback:\n",
    "    \"\"\"Callback class for tracking S3 upload progress.\"\"\"\n",
    "    \n",
    "    def __init__(self, filename, filesize):\n",
    "        self._filename = filename\n",
    "        self._size = filesize\n",
    "        self._seen_so_far = 0\n",
    "        self._pbar = tqdm(total=filesize, unit='B', unit_scale=True, desc=filename)\n",
    "    \n",
    "    def __call__(self, bytes_amount):\n",
    "        self._seen_so_far += bytes_amount\n",
    "        self._pbar.update(bytes_amount)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self._pbar.close()\n",
    "\n",
    "\n",
    "# Get S3 prefix for raw data (remove s3:// and bucket name)\n",
    "raw_data_s3_uri = settings.S3_PATHS['raw_data']\n",
    "s3_prefix = raw_data_s3_uri.replace(f's3://{bucket_name}/', '')\n",
    "\n",
    "print(f\"Uploading files to: {raw_data_s3_uri}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "upload_results = []\n",
    "\n",
    "for file_key, filename in settings.DATA_FILES.items():\n",
    "    local_file_path = dataset_dir / filename\n",
    "    s3_key = f\"{s3_prefix}{filename}\"\n",
    "    \n",
    "    if not local_file_path.exists():\n",
    "        print(f\"✗ Skipping {filename} - file not found locally\")\n",
    "        continue\n",
    "    \n",
    "    file_size = local_file_path.stat().st_size\n",
    "    \n",
    "    print(f\"\\nUploading: {filename}\")\n",
    "    print(f\"  Size: {file_size / (1024**2):.2f} MB\")\n",
    "    print(f\"  S3 Key: {s3_key}\")\n",
    "    \n",
    "    try:\n",
    "        # Upload with progress tracking\n",
    "        callback = S3ProgressCallback(filename, file_size)\n",
    "        s3_client.upload_file(\n",
    "            str(local_file_path),\n",
    "            bucket_name,\n",
    "            s3_key,\n",
    "            Callback=callback\n",
    "        )\n",
    "        \n",
    "        upload_results.append({\n",
    "            'File': filename,\n",
    "            'Size (MB)': f'{file_size / (1024**2):.2f}',\n",
    "            'S3 URI': f's3://{bucket_name}/{s3_key}',\n",
    "            'Status': '✓ Success'\n",
    "        })\n",
    "        \n",
    "        print(f\"  ✓ Upload complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Upload failed: {e}\")\n",
    "        upload_results.append({\n",
    "            'File': filename,\n",
    "            'Size (MB)': f'{file_size / (1024**2):.2f}',\n",
    "            'S3 URI': f's3://{bucket_name}/{s3_key}',\n",
    "            'Status': f'✗ Failed: {str(e)[:50]}'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nUpload Summary:\")\n",
    "df_uploads = pd.DataFrame(upload_results)\n",
    "print(df_uploads.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb90c42",
   "metadata": {},
   "source": [
    "## 6. Verify S3 Uploads\n",
    "\n",
    "List and verify all uploaded files in the S3 raw data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57089cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing objects in: s3://sagemaker-us-east-1-786869526001/aai540-group1/data/raw/\n",
      "======================================================================\n",
      "        File Size (MB)       Last Modified                              S3 Key\n",
      "airlines.csv      0.00 2026-01-23 07:57:56 aai540-group1/data/raw/airlines.csv\n",
      "airports.csv      0.02 2026-01-23 07:57:56 aai540-group1/data/raw/airports.csv\n",
      " flights.csv    564.96 2026-01-23 07:57:54  aai540-group1/data/raw/flights.csv\n",
      "\n",
      "======================================================================\n",
      "Total files in S3: 3\n",
      "Total size: 564.99 MB\n",
      "\n",
      "✓ All expected files are present in S3!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Listing objects in: {raw_data_s3_uri}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# List objects in the raw data prefix\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=s3_prefix\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    s3_files = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for obj in response['Contents']:\n",
    "        size_mb = obj['Size'] / (1024 * 1024)\n",
    "        total_size += obj['Size']\n",
    "        s3_files.append({\n",
    "            'File': obj['Key'].split('/')[-1],\n",
    "            'Size (MB)': f'{size_mb:.2f}',\n",
    "            'Last Modified': obj['LastModified'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'S3 Key': obj['Key']\n",
    "        })\n",
    "    \n",
    "    df_s3_files = pd.DataFrame(s3_files)\n",
    "    print(df_s3_files.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Total files in S3: {len(s3_files)}\")\n",
    "    print(f\"Total size: {total_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Verify all expected files are present\n",
    "    uploaded_filenames = {f['File'] for f in s3_files}\n",
    "    expected_filenames = set(settings.DATA_FILES.values())\n",
    "    \n",
    "    missing_files = expected_filenames - uploaded_filenames\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\n⚠ Warning: Missing files: {missing_files}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ All expected files are present in S3!\")\n",
    "        \n",
    "else:\n",
    "    print(\"✗ No objects found in the specified S3 location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a4af3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**S3 Datalake Setup Complete!**\n",
    "\n",
    "✓ Downloaded 2015 Flight Delays dataset from Kaggle  \n",
    "✓ Verified data integrity and file sizes  \n",
    "✓ Uploaded raw data to S3 datalake  \n",
    "✓ Confirmed all files are accessible in S3\n",
    "\n",
    "**Next Steps:**\n",
    "1. Set up Athena database and tables (next notebook)\n",
    "2. Perform exploratory data analysis\n",
    "3. Feature engineering and Feature Store setup\n",
    "\n",
    "**S3 Data Location:**\n",
    "```\n",
    "s3://{bucket}/aai540-group1/data/raw/\n",
    "├── flights.csv   (~580 MB, 5.8M rows)\n",
    "├── airlines.csv  (~1 KB, 14 airlines)\n",
    "└── airports.csv  (~30 KB, 322 airports)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
