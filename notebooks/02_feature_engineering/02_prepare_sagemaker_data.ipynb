{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08761483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "INSPECTING EXISTING PARQUET FEATURES\n",
      "Bucket: sagemaker-us-east-1-786869526001\n",
      "Prefix: aai540-group1/features/\n",
      "\n",
      "Files found:\n",
      "  No files found! Features need to be regenerated.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "# Get default bucket (where features were saved)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(\"INSPECTING EXISTING PARQUET FEATURES\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Prefix: aai540-group1/features/\")\n",
    "print()\n",
    "\n",
    "# List files in the features folder\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix='aai540-group1/features/')\n",
    "\n",
    "print(\"Files found:\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        size_mb = obj['Size'] / (1024 * 1024)\n",
    "        print(f\"  {obj['Key']} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  No files found! Features need to be regenerated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f15733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking PUBLIC bucket for features...\n",
      "Bucket: sagemaker-us-east-1-425709451100\n",
      "\n",
      "Features folder:\n",
      "  aai540-group1/features/prod_features.parquet (10.81 MB)\n",
      "  aai540-group1/features/test_features.parquet (10.58 MB)\n",
      "  aai540-group1/features/train_features.parquet (98.34 MB)\n",
      "  aai540-group1/features/val_features.parquet (7.64 MB)\n",
      "\n",
      "Raw data folder:\n",
      "  aai540-group1/data/raw/airlines.csv (0.00 MB)\n",
      "  aai540-group1/data/raw/airports.csv (0.02 MB)\n",
      "  aai540-group1/data/raw/flights.csv (564.96 MB)\n"
     ]
    }
   ],
   "source": [
    "# Public bucket (where raw data is stored)\n",
    "PUBLIC_BUCKET = \"sagemaker-us-east-1-425709451100\"\n",
    "\n",
    "print(\"Checking PUBLIC bucket for features...\")\n",
    "print(f\"Bucket: {PUBLIC_BUCKET}\")\n",
    "print()\n",
    "\n",
    "# Check for features folder\n",
    "response = s3.list_objects_v2(Bucket=PUBLIC_BUCKET, Prefix='aai540-group1/features/')\n",
    "\n",
    "print(\"Features folder:\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        size_mb = obj['Size'] / (1024 * 1024)\n",
    "        print(f\"  {obj['Key']} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  No features found in public bucket either.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Also check what's in raw data folder (confirm raw data exists)\n",
    "print(\"Raw data folder:\")\n",
    "response_raw = s3.list_objects_v2(Bucket=PUBLIC_BUCKET, Prefix='aai540-group1/data/raw/')\n",
    "if 'Contents' in response_raw:\n",
    "    for obj in response_raw['Contents']:\n",
    "        size_mb = obj['Size'] / (1024 * 1024)\n",
    "        print(f\"  {obj['Key']} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  No raw data found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec7a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING TRAINING FEATURES\n",
      "Loading: s3://sagemaker-us-east-1-425709451100/aai540-group1/features/train_features.parquet\n",
      "\n",
      "Loaded 4,299,046 rows × 27 columns\n",
      "\n",
      "Schema:\n",
      "MONTH                    int64\n",
      "DAY                      int64\n",
      "DAY_OF_WEEK              int64\n",
      "SCHEDULED_DEPARTURE      int64\n",
      "AIRLINE                 object\n",
      "ORIGIN_AIRPORT          object\n",
      "DESTINATION_AIRPORT     object\n",
      "DISTANCE                 int64\n",
      "SCHEDULED_TIME         float64\n",
      "DELAYED                  int64\n",
      "DEP_HOUR                 int64\n",
      "HOUR_SIN               float64\n",
      "HOUR_COS               float64\n",
      "IS_PEAK_HOUR             int64\n",
      "IS_WEEKEND               int64\n",
      "IS_LONG_HAUL             int64\n",
      "DISTANCE_BUCKET          int64\n",
      "ROUTE                   object\n",
      "AIRLINE_DELAY_RATE     float64\n",
      "ORIGIN_DELAY_RATE      float64\n",
      "DEST_DELAY_RATE        float64\n",
      "ROUTE_DELAY_RATE       float64\n",
      "ORIGIN_FLIGHTS         float64\n",
      "DEST_FLIGHTS           float64\n",
      "ROUTE_FLIGHTS          float64\n",
      "flight_id               object\n",
      "event_time             float64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "   MONTH  DAY  DAY_OF_WEEK  SCHEDULED_DEPARTURE AIRLINE ORIGIN_AIRPORT  \\\n",
      "0      1    1            4                    5      AS            ANC   \n",
      "1      1    1            4                   10      AA            LAX   \n",
      "2      1    1            4                   20      US            SFO   \n",
      "\n",
      "  DESTINATION_AIRPORT  DISTANCE  SCHEDULED_TIME  DELAYED  ...    ROUTE  \\\n",
      "0                 SEA      1448           205.0        0  ...  ANC_SEA   \n",
      "1                 PBI      2330           280.0        0  ...  LAX_PBI   \n",
      "2                 CLT      2296           286.0        0  ...  SFO_CLT   \n",
      "\n",
      "   AIRLINE_DELAY_RATE  ORIGIN_DELAY_RATE  DEST_DELAY_RATE  ROUTE_DELAY_RATE  \\\n",
      "0            0.123490           0.113604         0.153551          0.090405   \n",
      "1            0.186115           0.204192         0.223629          0.285714   \n",
      "2            0.179845           0.193393         0.153058          0.116247   \n",
      "\n",
      "   ORIGIN_FLIGHTS  DEST_FLIGHTS ROUTE_FLIGHTS                   flight_id  \\\n",
      "0        9.504203     11.410350      8.596004  2015-01-01-AS-ANC-SEA-0005   \n",
      "1       11.975848      9.785605      4.356709  2015-01-01-AA-LAX-PBI-0010   \n",
      "2       11.684026     11.296174      7.294377  2015-01-01-US-SFO-CLT-0020   \n",
      "\n",
      "     event_time  \n",
      "0  1.420070e+09  \n",
      "1  1.420070e+09  \n",
      "2  1.420070e+09  \n",
      "\n",
      "[3 rows x 27 columns]\n",
      "\n",
      "Target distribution:\n",
      "Delayed: 805,372 (18.73%)\n",
      "On-time: 3,493,674 (81.27%)\n",
      "\n",
      "Missing values:\n",
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"LOADING TRAINING FEATURES\")\n",
    "\n",
    "# Load training features from public bucket\n",
    "train_path = f\"s3://{PUBLIC_BUCKET}/aai540-group1/features/train_features.parquet\"\n",
    "print(f\"Loading: {train_path}\")\n",
    "\n",
    "df_train = pd.read_parquet(train_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_train):,} rows × {df_train.shape[1]} columns\")\n",
    "print(\"\\nSchema:\")\n",
    "print(df_train.dtypes)\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_train.head(3))\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "if 'DELAYED' in df_train.columns:\n",
    "    delay_rate = df_train['DELAYED'].mean() * 100\n",
    "    print(f\"Delayed: {(df_train['DELAYED']==1).sum():,} ({delay_rate:.2f}%)\")\n",
    "    print(f\"On-time: {(df_train['DELAYED']==0).sum():,} ({100-delay_rate:.2f}%)\")\n",
    "else:\n",
    "    print(\"WARNING: DELAYED column not found!\")\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing = df_train.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e5409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFYING ALL SPLITS\n",
      "\n",
      "Loading val...\n",
      "  Rows: 482,878\n",
      "  Columns: 27\n",
      "  Delayed: 57,237 (11.85%)\n",
      "  Missing values: 0\n",
      "  Schema matches train: True\n",
      "\n",
      "Loading test...\n",
      "  Rows: 462,367\n",
      "  Columns: 27\n",
      "  Delayed: 67,576 (14.62%)\n",
      "  Missing values: 0\n",
      "  Schema matches train: True\n",
      "\n",
      "Loading prod...\n",
      "  Rows: 469,717\n",
      "  Columns: 27\n",
      "  Delayed: 93,313 (19.87%)\n",
      "  Missing values: 0\n",
      "  Schema matches train: True\n",
      "\n",
      "SUMMARY\n",
      "\n",
      "Split        Rows         Delay Rate   Columns    Status\n",
      "------------------------------------------------------------\n",
      "train        4,299,046    18.73      % 27         OK\n",
      "val          482,878      11.85      % 27         OK\n",
      "test         462,367      14.62      % 27         OK\n",
      "prod         469,717      19.87      % 27         OK\n",
      "\n",
      "All splits verified and ready for training!\n"
     ]
    }
   ],
   "source": [
    "print(\"VERIFYING ALL SPLITS\")\n",
    "\n",
    "splits = ['val', 'test', 'prod']\n",
    "datasets = {}\n",
    "\n",
    "for split in splits:\n",
    "    path = f\"s3://{PUBLIC_BUCKET}/aai540-group1/features/{split}_features.parquet\"\n",
    "    print(f\"\\nLoading {split}...\")\n",
    "    df = pd.read_parquet(path)\n",
    "    datasets[split] = df\n",
    "    \n",
    "    delay_rate = df['DELAYED'].mean() * 100\n",
    "    n_delayed = (df['DELAYED'] == 1).sum()\n",
    "    n_ontime = (df['DELAYED'] == 0).sum()\n",
    "    \n",
    "    print(f\"  Rows: {len(df):,}\")\n",
    "    print(f\"  Columns: {df.shape[1]}\")\n",
    "    print(f\"  Delayed: {n_delayed:,} ({delay_rate:.2f}%)\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Schema matches train: {list(df.columns) == list(df_train.columns)}\")\n",
    "\n",
    "print(\"\\nSUMMARY\")\n",
    "print(f\"\\n{'Split':<12} {'Rows':<12} {'Delay Rate':<12} {'Columns':<10} {'Status'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "all_datasets = {'train': df_train, **datasets}\n",
    "for split, df in all_datasets.items():\n",
    "    delay_rate = df['DELAYED'].mean() * 100\n",
    "    status = \"OK\" if df.isnull().sum().sum() == 0 else \"WARNING\"\n",
    "    print(f\"{split:<12} {len(df):<12,} {delay_rate:<11.2f}% {df.shape[1]:<10} {status}\")\n",
    "\n",
    "print(\"\\nAll splits verified and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c09bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA FOR SAGEMAKER XGBOOST\n",
      "\n",
      "Using 20 engineered features (matching section 6)\n",
      "Target: DELAYED\n",
      "\n",
      "Extracting features and target...\n",
      "  train     4,299,046 rows × 21 columns (1 target + 20 features)\n",
      "  val         482,878 rows × 21 columns (1 target + 20 features)\n",
      "  test        462,367 rows × 21 columns (1 target + 20 features)\n",
      "\n",
      "Sample data (train, first 3 rows):\n",
      "   DELAYED  MONTH  DAY  DAY_OF_WEEK  DEP_HOUR  SCHEDULED_DEPARTURE  HOUR_SIN  \\\n",
      "0        0      1    1            4         0                    5       0.0   \n",
      "1        0      1    1            4         0                   10       0.0   \n",
      "2        0      1    1            4         0                   20       0.0   \n",
      "\n",
      "   HOUR_COS  IS_PEAK_HOUR  IS_WEEKEND  ...  SCHEDULED_TIME  IS_LONG_HAUL  \\\n",
      "0       1.0             0           0  ...           205.0             0   \n",
      "1       1.0             0           0  ...           280.0             1   \n",
      "2       1.0             0           0  ...           286.0             1   \n",
      "\n",
      "   DISTANCE_BUCKET  AIRLINE_DELAY_RATE  ORIGIN_DELAY_RATE  DEST_DELAY_RATE  \\\n",
      "0                1            0.123490           0.113604         0.153551   \n",
      "1                2            0.186115           0.204192         0.223629   \n",
      "2                2            0.179845           0.193393         0.153058   \n",
      "\n",
      "   ROUTE_DELAY_RATE  ORIGIN_FLIGHTS  DEST_FLIGHTS  ROUTE_FLIGHTS  \n",
      "0          0.090405        9.504203     11.410350       8.596004  \n",
      "1          0.285714       11.975848      9.785605       4.356709  \n",
      "2          0.116247       11.684026     11.296174       7.294377  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Data ready for SageMaker XGBoost format (CSV, target first, no headers)\n"
     ]
    }
   ],
   "source": [
    "print(\"PREPARING DATA FOR SAGEMAKER XGBOOST\")\n",
    "\n",
    "# Feature columns from section 6 (20 engineered features - all numeric)\n",
    "FEATURE_COLS = [\n",
    "    # Temporal\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND',\n",
    "    \n",
    "    # Distance\n",
    "    'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET',\n",
    "    \n",
    "    # Target-encoded\n",
    "    'AIRLINE_DELAY_RATE', 'ORIGIN_DELAY_RATE', 'DEST_DELAY_RATE', 'ROUTE_DELAY_RATE',\n",
    "    \n",
    "    # Volume\n",
    "    'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS'\n",
    "]\n",
    "\n",
    "TARGET_COL = 'DELAYED'\n",
    "\n",
    "print(f\"\\nUsing {len(FEATURE_COLS)} engineered features (matching section 6)\")\n",
    "print(f\"Target: {TARGET_COL}\")\n",
    "\n",
    "# Prepare datasets for SageMaker (target first, no headers)\n",
    "print(\"\\nExtracting features and target...\")\n",
    "\n",
    "# Add validation set to datasets dict\n",
    "datasets['val'] = datasets.pop('val')  # Ensure val is in correct order\n",
    "all_data = {'train': df_train, 'val': datasets['val'], 'test': datasets['test']}\n",
    "\n",
    "prepared_data = {}\n",
    "for split, df in all_data.items():\n",
    "    # SageMaker format: target column first, then features\n",
    "    cols = [TARGET_COL] + FEATURE_COLS\n",
    "    df_prep = df[cols].copy()\n",
    "    prepared_data[split] = df_prep\n",
    "    \n",
    "    print(f\"  {split:<8} {df_prep.shape[0]:>10,} rows × {df_prep.shape[1]:>2} columns (1 target + {len(FEATURE_COLS)} features)\")\n",
    "\n",
    "print(\"\\nSample data (train, first 3 rows):\")\n",
    "print(prepared_data['train'].head(3))\n",
    "\n",
    "print(\"\\nData ready for SageMaker XGBoost format (CSV, target first, no headers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4247f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING DATA TO S3\n",
      "\n",
      "Bucket: sagemaker-us-east-1-786869526001\n",
      "Prefix: aai540-group1/training/engineered-baseline\n",
      "\n",
      "Checking S3 for existing data...\n",
      "  train    missing\n",
      "  val      missing\n",
      "  test     missing\n",
      "\n",
      "Saving CSV files locally...\n",
      "  train    /tmp/sagemaker_data/train.csv (842.15 MB)\n",
      "  val      /tmp/sagemaker_data/val.csv (75.64 MB)\n",
      "  test     /tmp/sagemaker_data/test.csv (91.01 MB)\n",
      "\n",
      "Uploading to S3...\n",
      "  train    -> s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/train/train.csv\n",
      "  val      -> s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/val/val.csv\n",
      "  test     -> s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/test/test.csv\n",
      "\n",
      "DATA READY\n",
      "\n",
      "Training data:   s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/train/train.csv\n",
      "Validation data: s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/val/val.csv\n",
      "Test data:       s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-baseline/test/test.csv\n",
      "\n",
      "Ready for SageMaker training!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"UPLOADING DATA TO S3\")\n",
    "\n",
    "# S3 paths (fixed path - no timestamp for idempotency)\n",
    "prefix = \"aai540-group1/training/engineered-baseline\"\n",
    "training_prefix = prefix\n",
    "\n",
    "print(f\"\\nBucket: {bucket}\")\n",
    "print(f\"Prefix: {training_prefix}\")\n",
    "\n",
    "# Check if data already exists in S3\n",
    "print(\"\\nChecking S3 for existing data...\")\n",
    "s3_paths = {}\n",
    "upload_needed = {}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    s3_key = f\"{training_prefix}/{split}/{split}.csv\"\n",
    "    s3_path = f\"s3://{bucket}/{s3_key}\"\n",
    "    s3_paths[split] = s3_path\n",
    "    \n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=s3_key)\n",
    "        print(f\"  {split:<8} exists: {s3_path}\")\n",
    "        upload_needed[split] = False\n",
    "    except:\n",
    "        print(f\"  {split:<8} missing\")\n",
    "        upload_needed[split] = True\n",
    "\n",
    "# Upload only if needed\n",
    "if any(upload_needed.values()):\n",
    "    # Save locally as CSV (no headers, no index)\n",
    "    local_dir = \"/tmp/sagemaker_data\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nSaving CSV files locally...\")\n",
    "    for split, df in prepared_data.items():\n",
    "        if upload_needed[split]:\n",
    "            local_path = f\"{local_dir}/{split}.csv\"\n",
    "            df.to_csv(local_path, header=False, index=False)\n",
    "            size_mb = os.path.getsize(local_path) / (1024 * 1024)\n",
    "            print(f\"  {split:<8} {local_path} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    print(\"\\nUploading to S3...\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        if upload_needed[split]:\n",
    "            local_path = f\"{local_dir}/{split}.csv\"\n",
    "            s3_key = f\"{training_prefix}/{split}/{split}.csv\"\n",
    "            s3.upload_file(local_path, bucket, s3_key)\n",
    "            print(f\"  {split:<8} -> {s3_paths[split]}\")\n",
    "else:\n",
    "    print(\"\\nAll data already exists in S3, skipping upload\")\n",
    "\n",
    "print(\"\\nDATA READY\")\n",
    "print(f\"\\nTraining data:   {s3_paths['train']}\")\n",
    "print(f\"Validation data: {s3_paths['val']}\")\n",
    "print(f\"Test data:       {s3_paths['test']}\")\n",
    "print(f\"\\nReady for SageMaker training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ab132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
