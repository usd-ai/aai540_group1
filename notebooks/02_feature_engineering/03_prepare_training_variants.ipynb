{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51aca1cf",
   "metadata": {},
   "source": [
    "# Prepare Training Data Variants\n",
    "\n",
    "## Purpose\n",
    "Prepare CSV training data for SageMaker XGBoost experiments with different feature subsets.\n",
    "This notebook creates two additional variants beyond the 20-feature \"engineered-baseline\":\n",
    "1. **Raw baseline (6 features)**: Minimal features for benchmark model\n",
    "2. **Engineered without target encoding (16 features)**: Avoids temporal leakage from target-encoded rates\n",
    "\n",
    "## Prerequisites\n",
    "- Feature parquet files available in public S3 bucket\n",
    "- Run after 02_prepare_sagemaker_data.ipynb (which creates the 20-feature variant)\n",
    "\n",
    "## Output Locations\n",
    "- `s3://{PERSONAL_BUCKET}/aai540-group1/training/raw-baseline/`\n",
    "- `s3://{PERSONAL_BUCKET}/aai540-group1/training/engineered-no-target-encoding/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6dd3fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Configuration:\n",
      "  Public bucket (source): s3://sagemaker-us-east-1-425709451100/aai540-group1/features/\n",
      "  Personal bucket (dest):  s3://sagemaker-us-east-1-786869526001/\n",
      "\n",
      "Feature sets:\n",
      "  raw-baseline                        -  6 features: 6 raw features for benchmark model\n",
      "  engineered-no-target-encoding       - 16 features: 16 features excluding target-encoded rates\n",
      "\n",
      "Target: DELAYED\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# S3 buckets\n",
    "PUBLIC_BUCKET = 'sagemaker-us-east-1-425709451100'\n",
    "PERSONAL_BUCKET = sess.default_bucket()\n",
    "PARQUET_PREFIX = 'aai540-group1/features'\n",
    "\n",
    "# Target and feature definitions\n",
    "TARGET = 'DELAYED'\n",
    "\n",
    "# 6 raw features (for benchmark model)\n",
    "RAW_FEATURES = [\n",
    "    'MONTH', 'DAY_OF_WEEK', 'DEP_HOUR', \n",
    "    'DISTANCE', 'SCHEDULED_TIME', 'IS_WEEKEND'\n",
    "]\n",
    "\n",
    "# 16 engineered features WITHOUT target encoding\n",
    "# Excludes: AIRLINE_DELAY_RATE, ORIGIN_DELAY_RATE, DEST_DELAY_RATE, ROUTE_DELAY_RATE\n",
    "ENGINEERED_NO_TARGET_ENCODING = [\n",
    "    # Temporal (9)\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND',\n",
    "    # Distance (4)\n",
    "    'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET',\n",
    "    # Volume (3)\n",
    "    'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS'\n",
    "]\n",
    "\n",
    "# Feature set configurations\n",
    "FEATURE_SETS = {\n",
    "    'raw-baseline': {\n",
    "        'features': RAW_FEATURES,\n",
    "        'prefix': 'aai540-group1/training/raw-baseline',\n",
    "        'description': '6 raw features for benchmark model'\n",
    "    },\n",
    "    'engineered-no-target-encoding': {\n",
    "        'features': ENGINEERED_NO_TARGET_ENCODING,\n",
    "        'prefix': 'aai540-group1/training/engineered-no-target-encoding',\n",
    "        'description': '16 features excluding target-encoded rates'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Split name mapping (parquet uses 'val', SageMaker convention is 'validation')\n",
    "SPLIT_MAPPING = {\n",
    "    'train': 'train',\n",
    "    'val': 'validation',\n",
    "    'test': 'test'\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Public bucket (source): s3://{PUBLIC_BUCKET}/{PARQUET_PREFIX}/\")\n",
    "print(f\"  Personal bucket (dest):  s3://{PERSONAL_BUCKET}/\")\n",
    "print(f\"\\nFeature sets:\")\n",
    "for name, config in FEATURE_SETS.items():\n",
    "    print(f\"  {name:35s} - {len(config['features']):2d} features: {config['description']}\")\n",
    "print(f\"\\nTarget: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae27551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking S3 for existing files...\n",
      "\n",
      "Feature Set                              Train      Validation   Test      \n",
      "---------------------------------------------------------------------------\n",
      "raw-baseline                             ✓          ✓            ✓         \n",
      "engineered-no-target-encoding            missing    missing      missing   \n",
      "\n",
      "Processing needed for: engineered-no-target-encoding\n"
     ]
    }
   ],
   "source": [
    "# Check existing data in S3\n",
    "print(\"Checking S3 for existing files...\\n\")\n",
    "\n",
    "upload_status = {}\n",
    "\n",
    "for feature_set_name, config in FEATURE_SETS.items():\n",
    "    upload_status[feature_set_name] = {}\n",
    "    \n",
    "    for parquet_split, s3_split in SPLIT_MAPPING.items():\n",
    "        s3_key = f\"{config['prefix']}/{s3_split}/{s3_split}.csv\"\n",
    "        \n",
    "        try:\n",
    "            s3_client.head_object(Bucket=PERSONAL_BUCKET, Key=s3_key)\n",
    "            upload_status[feature_set_name][parquet_split] = 'exists'\n",
    "        except ClientError:\n",
    "            upload_status[feature_set_name][parquet_split] = 'missing'\n",
    "\n",
    "# Print status table\n",
    "print(f\"{'Feature Set':<40s} {'Train':<10s} {'Validation':<12s} {'Test':<10s}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for feature_set_name in FEATURE_SETS.keys():\n",
    "    statuses = upload_status[feature_set_name]\n",
    "    train_status = '✓' if statuses['train'] == 'exists' else 'missing'\n",
    "    val_status = '✓' if statuses['val'] == 'exists' else 'missing'\n",
    "    test_status = '✓' if statuses['test'] == 'exists' else 'missing'\n",
    "    \n",
    "    print(f\"{feature_set_name:<40s} {train_status:<10s} {val_status:<12s} {test_status:<10s}\")\n",
    "\n",
    "# Determine what needs processing\n",
    "needs_processing = {}\n",
    "for feature_set_name, statuses in upload_status.items():\n",
    "    if any(status == 'missing' for status in statuses.values()):\n",
    "        needs_processing[feature_set_name] = FEATURE_SETS[feature_set_name]\n",
    "\n",
    "if needs_processing:\n",
    "    print(f\"\\nProcessing needed for: {', '.join(needs_processing.keys())}\")\n",
    "else:\n",
    "    print(\"\\n✓ All files already exist in S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b542bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet files and preparing CSVs...\n",
      "\n",
      "Processing engineered-no-target-encoding (16 features)...\n",
      "  train       : 4,299,046 rows prepared\n",
      "  validation  : 482,878 rows prepared\n",
      "  test        : 462,367 rows prepared\n",
      "\n",
      "✓ CSV files prepared in /tmp/training_variants\n"
     ]
    }
   ],
   "source": [
    "# Load parquet and prepare CSVs\n",
    "if not needs_processing:\n",
    "    print(\"No processing needed - all files exist in S3\")\n",
    "else:\n",
    "    print(\"Loading parquet files and preparing CSVs...\\n\")\n",
    "    \n",
    "    # Local temp directory\n",
    "    local_dir = '/tmp/training_variants'\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    # Track prepared files\n",
    "    prepared_files = {}\n",
    "    \n",
    "    for feature_set_name, config in needs_processing.items():\n",
    "        print(f\"Processing {feature_set_name} ({len(config['features'])} features)...\")\n",
    "        prepared_files[feature_set_name] = {}\n",
    "        \n",
    "        for parquet_split, s3_split in SPLIT_MAPPING.items():\n",
    "            # Skip if already exists\n",
    "            if upload_status[feature_set_name][parquet_split] == 'exists':\n",
    "                print(f\"  {s3_split:12s}: Already exists, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Load parquet with columnar projection (only needed columns)\n",
    "            s3_path = f\"s3://{PUBLIC_BUCKET}/{PARQUET_PREFIX}/{parquet_split}_features.parquet\"\n",
    "            columns_to_read = [TARGET] + config['features']\n",
    "            df = pd.read_parquet(s3_path, columns=columns_to_read)\n",
    "            \n",
    "            # Reorder: target first, then features\n",
    "            df_train = df[[TARGET] + config['features']].copy()\n",
    "            \n",
    "            # Save locally as CSV (no headers, no index)\n",
    "            local_path = f\"{local_dir}/{feature_set_name}_{s3_split}.csv\"\n",
    "            df_train.to_csv(local_path, header=False, index=False)\n",
    "            \n",
    "            prepared_files[feature_set_name][parquet_split] = {\n",
    "                'local_path': local_path,\n",
    "                's3_split': s3_split,\n",
    "                'rows': len(df_train)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {s3_split:12s}: {len(df_train):>7,} rows prepared\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(f\"✓ CSV files prepared in {local_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923993c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to S3...\n",
      "\n",
      "engineered-no-target-encoding:\n",
      "  train       : Uploaded to s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/train/train.csv\n",
      "  validation  : Uploaded to s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/validation/validation.csv\n",
      "  test        : Uploaded to s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/test/test.csv\n",
      "\n",
      "✓ Upload complete\n",
      "\n",
      "All S3 paths:\n",
      "\n",
      "raw-baseline:\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/raw-baseline/train/train.csv\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/raw-baseline/validation/validation.csv\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/raw-baseline/test/test.csv\n",
      "\n",
      "engineered-no-target-encoding:\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/train/train.csv\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/validation/validation.csv\n",
      "  s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "if not needs_processing:\n",
    "    print(\"No uploads needed - all files already exist in S3\")\n",
    "else:\n",
    "    print(\"Uploading to S3...\\n\")\n",
    "    \n",
    "    uploaded_paths = []\n",
    "    \n",
    "    for feature_set_name, files in prepared_files.items():\n",
    "        config = FEATURE_SETS[feature_set_name]\n",
    "        print(f\"{feature_set_name}:\")\n",
    "        \n",
    "        for parquet_split, file_info in files.items():\n",
    "            s3_split = file_info['s3_split']\n",
    "            s3_key = f\"{config['prefix']}/{s3_split}/{s3_split}.csv\"\n",
    "            s3_path = f\"s3://{PERSONAL_BUCKET}/{s3_key}\"\n",
    "            \n",
    "            # Double-check if exists (idempotency)\n",
    "            try:\n",
    "                s3_client.head_object(Bucket=PERSONAL_BUCKET, Key=s3_key)\n",
    "                print(f\"  {s3_split:12s}: Already exists at {s3_path}\")\n",
    "            except ClientError:\n",
    "                s3_client.upload_file(file_info['local_path'], PERSONAL_BUCKET, s3_key)\n",
    "                print(f\"  {s3_split:12s}: Uploaded to {s3_path}\")\n",
    "            \n",
    "            uploaded_paths.append(s3_path)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(f\"✓ Upload complete\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\nAll S3 paths:\")\n",
    "    for feature_set_name, config in FEATURE_SETS.items():\n",
    "        print(f\"\\n{feature_set_name}:\")\n",
    "        for s3_split in ['train', 'validation', 'test']:\n",
    "            s3_path = f\"s3://{PERSONAL_BUCKET}/{config['prefix']}/{s3_split}/{s3_split}.csv\"\n",
    "            print(f\"  {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b66b1",
   "metadata": {},
   "source": [
    "## Data Ready\n",
    "\n",
    "Training data variants prepared for SageMaker XGBoost:\n",
    "\n",
    "| Variant | Features | S3 Location |\n",
    "|---------|----------|-------------|\n",
    "| raw-baseline | 6 | `s3://{bucket}/aai540-group1/training/raw-baseline/` |\n",
    "| engineered-no-target-encoding | 16 | `s3://{bucket}/aai540-group1/training/engineered-no-target-encoding/` |\n",
    "| engineered-baseline | 20 | `s3://{bucket}/aai540-group1/training/engineered-baseline/` (from 02_prepare_sagemaker_data.ipynb) |\n",
    "\n",
    "Each location contains:\n",
    "- `train/train.csv`\n",
    "- `validation/validation.csv`  \n",
    "- `test/test.csv`\n",
    "\n",
    "Format: CSV with target column first, no headers (SageMaker XGBoost format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
