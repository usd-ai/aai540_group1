{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2435dc",
   "metadata": {},
   "source": [
    "# Engineered Baseline Model (No Target Encoding)\n",
    "\n",
    "## Purpose\n",
    "This notebook trains an XGBoost model on **16 engineered features** excluding target-encoded rates to avoid temporal leakage. This model uses temporal transformations, distance features, and volume metrics while maintaining proper train/test separation.\n",
    "\n",
    "## Prerequisites\n",
    "- Model registry (`flight-delay-models` group) must exist\n",
    "- Training data prepared by running `02_feature_engineering/03_prepare_training_variants.ipynb`\n",
    "- Data location: `s3://{PERSONAL_BUCKET}/aai540-group1/training/engineered-no-target-encoding/`\n",
    "\n",
    "## Comparison Intent\n",
    "This model serves as a middle ground between:\n",
    "- **Raw baseline** (6 features): Basic temporal and flight characteristics\n",
    "- **Full engineered** (20 features): Includes target-encoded historical delay rates\n",
    "\n",
    "By excluding target-encoded features (AIRLINE_DELAY_RATE, ORIGIN_DELAY_RATE, DEST_DELAY_RATE, ROUTE_DELAY_RATE), we evaluate performance without potential information leakage while still leveraging feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85753291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n",
      "SageMaker version: 2.245.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "print(\"Imports loaded successfully\")\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7b1807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: LabRole\n",
      "Public bucket (features): s3://sagemaker-us-east-1-425709451100/aai540-group1/features/\n",
      "Personal bucket (training): s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/\n",
      "\n",
      "Engineered features (16): ['MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE', 'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND', 'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET', 'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS']\n",
      "Target: DELAYED\n"
     ]
    }
   ],
   "source": [
    "# SageMaker session and role\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# S3 buckets\n",
    "PUBLIC_BUCKET = 'sagemaker-us-east-1-425709451100'\n",
    "PERSONAL_BUCKET = sess.default_bucket()\n",
    "\n",
    "# Engineered features (16 total - excluding target-encoded rates)\n",
    "ENGINEERED_FEATURES = [\n",
    "    # Temporal (9)\n",
    "    'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND',\n",
    "    # Distance (4)\n",
    "    'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET',\n",
    "    # Volume (3)\n",
    "    'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS'\n",
    "]\n",
    "TARGET = 'DELAYED'\n",
    "\n",
    "# S3 paths\n",
    "INPUT_PREFIX = 'aai540-group1/features'\n",
    "OUTPUT_PREFIX = 'aai540-group1/training/engineered-no-target-encoding'\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role.split('/')[-1]}\")\n",
    "print(f\"Public bucket (features): s3://{PUBLIC_BUCKET}/{INPUT_PREFIX}/\")\n",
    "print(f\"Personal bucket (training): s3://{PERSONAL_BUCKET}/{OUTPUT_PREFIX}/\")\n",
    "print(f\"\\nEngineered features ({len(ENGINEERED_FEATURES)}): {ENGINEERED_FEATURES}\")\n",
    "print(f\"Target: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9c7d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature data from public bucket...\n",
      "train: 4,299,046 rows × 17 columns\n",
      "val  : 482,878 rows × 17 columns\n",
      "\n",
      "Total samples: 4,781,924\n",
      "\n",
      "Features: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE', 'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND', 'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET', 'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS']\n",
      "\n",
      "Sample from training set:\n",
      "   MONTH  DAY  DAY_OF_WEEK  DEP_HOUR  SCHEDULED_DEPARTURE  HOUR_SIN  HOUR_COS  \\\n",
      "0      1    1            4         0                    5       0.0       1.0   \n",
      "1      1    1            4         0                   10       0.0       1.0   \n",
      "2      1    1            4         0                   20       0.0       1.0   \n",
      "\n",
      "   IS_PEAK_HOUR  IS_WEEKEND  DISTANCE  SCHEDULED_TIME  IS_LONG_HAUL  \\\n",
      "0             0           0      1448           205.0             0   \n",
      "1             0           0      2330           280.0             1   \n",
      "2             0           0      2296           286.0             1   \n",
      "\n",
      "   DISTANCE_BUCKET  ORIGIN_FLIGHTS  DEST_FLIGHTS  ROUTE_FLIGHTS  DELAYED  \n",
      "0                1        9.504203     11.410350       8.596004        0  \n",
      "1                2       11.975848      9.785605       4.356709        0  \n",
      "2                2       11.684026     11.296174       7.294377        0  \n"
     ]
    }
   ],
   "source": [
    "# Load feature data from public bucket\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "print(\"Loading feature data from public bucket...\")\n",
    "\n",
    "datasets = {}\n",
    "for split in ['train', 'val']:\n",
    "    s3_path = f\"s3://{PUBLIC_BUCKET}/{INPUT_PREFIX}/{split}_features.parquet\"\n",
    "    \n",
    "    # Read only needed columns from parquet (efficient columnar read)\n",
    "    # Parquet's columnar format allows reading specific columns without loading entire file\n",
    "    columns_to_read = ENGINEERED_FEATURES + [TARGET]\n",
    "    df_engineered = pd.read_parquet(s3_path, columns=columns_to_read)\n",
    "    \n",
    "    datasets[split] = df_engineered\n",
    "    \n",
    "    print(f\"{split:5s}: {df_engineered.shape[0]:>7,} rows × {df_engineered.shape[1]} columns\")\n",
    "\n",
    "# Unpack datasets\n",
    "train_df = datasets['train']\n",
    "val_df = datasets['val']\n",
    "\n",
    "print(f\"\\nTotal samples: {sum(df.shape[0] for df in datasets.values()):,}\")\n",
    "print(f\"\\nFeatures: {ENGINEERED_FEATURES}\")\n",
    "print(f\"\\nSample from training set:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff80046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Estimator configured\n",
      "\n",
      "Container: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1\n",
      "Instance type: ml.m5.xlarge\n",
      "Output path: s3://sagemaker-us-east-1-786869526001/aai540-group1/models/engineered-no-target-encoding\n",
      "\n",
      "Hyperparameters:\n",
      "  objective                 = binary:logistic\n",
      "  eval_metric               = auc\n",
      "  max_depth                 = 8\n",
      "  eta                       = 0.05\n",
      "  num_round                 = 1000\n",
      "  scale_pos_weight          = 4.58\n",
      "  subsample                 = 0.8\n",
      "  colsample_bytree          = 0.8\n",
      "  early_stopping_rounds     = 50\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost Estimator\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "# Get XGBoost container image\n",
    "container = retrieve('xgboost', region, version='1.5-1')\n",
    "\n",
    "# Output path for model artifacts\n",
    "output_path = f\"s3://{PERSONAL_BUCKET}/aai540-group1/models/engineered-no-target-encoding\"\n",
    "\n",
    "# Hyperparameters for engineered baseline model\n",
    "hyperparameters = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.05,\n",
    "    'num_round': 1000,\n",
    "    'scale_pos_weight': 4.58,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "# Create estimator\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "print(\"XGBoost Estimator configured\")\n",
    "print(f\"\\nContainer: {container}\")\n",
    "print(f\"Instance type: ml.m5.xlarge\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key:25s} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7dfb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-prepared training data:\n",
      "  Train:      s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/train/train.csv\n",
      "  Validation: s3://sagemaker-us-east-1-786869526001/aai540-group1/training/engineered-no-target-encoding/validation/validation.csv\n",
      "  ✓ Train data verified\n",
      "  ✓ Validation data verified\n"
     ]
    }
   ],
   "source": [
    "# Use pre-prepared training data (from 03_prepare_training_variants.ipynb)\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "train_path = f\"s3://{PERSONAL_BUCKET}/{OUTPUT_PREFIX}/train/train.csv\"\n",
    "val_path = f\"s3://{PERSONAL_BUCKET}/{OUTPUT_PREFIX}/validation/validation.csv\"\n",
    "\n",
    "print(\"Using pre-prepared training data:\")\n",
    "print(f\"  Train:      {train_path}\")\n",
    "print(f\"  Validation: {val_path}\")\n",
    "\n",
    "# Verify data exists\n",
    "for name, path in [('Train', train_path), ('Validation', val_path)]:\n",
    "    key = path.replace(f\"s3://{PERSONAL_BUCKET}/\", \"\")\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=PERSONAL_BUCKET, Key=key)\n",
    "        print(f\"  ✓ {name} data verified\")\n",
    "    except ClientError:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{name} data not found at {path}\\n\"\n",
    "            \"Run 02_feature_engineering/03_prepare_training_variants.ipynb first.\"\n",
    "        )\n",
    "\n",
    "# Create TrainingInput channels\n",
    "train_input = TrainingInput(train_path, content_type='text/csv')\n",
    "validation_input = TrainingInput(val_path, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a401904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2026-02-02-06-15-34-211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training job...\n",
      "2026-02-02 06:15:34 Starting - Starting the training job...\n",
      "2026-02-02 06:16:00 Starting - Preparing the instances for training...\n",
      "2026-02-02 06:16:25 Downloading - Downloading input data...\n",
      "2026-02-02 06:17:01 Downloading - Downloading the training image...../miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[2026-02-02 06:17:50.153 ip-10-0-244-215.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2026-02-02 06:17:50.176 ip-10-0-244-215.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2026-02-02:06:17:50:INFO] Imported framework sagemaker_xgboost_container.training\n",
      "[2026-02-02:06:17:50:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\n",
      "Returning the value itself\n",
      "[2026-02-02:06:17:50:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\n",
      "Returning the value itself\n",
      "[2026-02-02:06:17:50:INFO] No GPUs detected (normal if no gpus installed)\n",
      "[2026-02-02:06:17:50:INFO] Running XGBoost Sagemaker in algorithm mode\n",
      "[2026-02-02:06:17:50:INFO] Determined 0 GPU(s) available on the instance.\n",
      "[2026-02-02:06:17:50:INFO] Determined delimiter of CSV input is ','\n",
      "[2026-02-02:06:17:50:INFO] Determined delimiter of CSV input is ','\n",
      "[2026-02-02:06:17:50:INFO] files path: /opt/ml/input/data/train\n",
      "[2026-02-02:06:17:50:INFO] Determined delimiter of CSV input is ','\n",
      "[2026-02-02:06:17:53:INFO] files path: /opt/ml/input/data/validation\n",
      "[2026-02-02:06:17:53:INFO] Determined delimiter of CSV input is ','\n",
      "[2026-02-02:06:17:53:INFO] Single node training.\n",
      "[2026-02-02:06:17:53:INFO] Train matrix has 4299046 rows and 16 columns\n",
      "[2026-02-02:06:17:53:INFO] Validation matrix has 482878 rows\n",
      "[2026-02-02 06:17:53.894 ip-10-0-244-215.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2026-02-02 06:17:53.895 ip-10-0-244-215.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2026-02-02 06:17:53.896 ip-10-0-244-215.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2026-02-02 06:17:53.896 ip-10-0-244-215.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2026-02-02:06:17:53:INFO] Debug hook created from config\n",
      "\n",
      "2026-02-02 06:17:41 Training - Training image download completed. Training in progress.[0]#011train-auc:0.64238#011validation-auc:0.60063\n",
      "[2026-02-02 06:18:01.180 ip-10-0-244-215.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\n",
      "[2026-02-02 06:18:01.182 ip-10-0-244-215.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\n",
      "[1]#011train-auc:0.65102#011validation-auc:0.60285\n",
      "[2]#011train-auc:0.66124#011validation-auc:0.60353\n",
      "[3]#011train-auc:0.66335#011validation-auc:0.59975\n",
      "[4]#011train-auc:0.66505#011validation-auc:0.59452\n",
      "[5]#011train-auc:0.66641#011validation-auc:0.59471\n",
      "[6]#011train-auc:0.66699#011validation-auc:0.59438\n",
      "[7]#011train-auc:0.66746#011validation-auc:0.59877\n",
      "[8]#011train-auc:0.66830#011validation-auc:0.59857\n",
      "[9]#011train-auc:0.67017#011validation-auc:0.59783\n",
      "[10]#011train-auc:0.67137#011validation-auc:0.59622\n",
      "[11]#011train-auc:0.67247#011validation-auc:0.59589\n",
      "[12]#011train-auc:0.67269#011validation-auc:0.59796\n",
      "[13]#011train-auc:0.67274#011validation-auc:0.59944\n",
      "[14]#011train-auc:0.67303#011validation-auc:0.59884\n",
      "[15]#011train-auc:0.67351#011validation-auc:0.59857\n",
      "[16]#011train-auc:0.67453#011validation-auc:0.59726\n",
      "[17]#011train-auc:0.67564#011validation-auc:0.59686\n",
      "[18]#011train-auc:0.67604#011validation-auc:0.59484\n",
      "[19]#011train-auc:0.67688#011validation-auc:0.59376\n",
      "[20]#011train-auc:0.67724#011validation-auc:0.59357\n",
      "[21]#011train-auc:0.67821#011validation-auc:0.59249\n",
      "[22]#011train-auc:0.67870#011validation-auc:0.59375\n",
      "[23]#011train-auc:0.67942#011validation-auc:0.59274\n",
      "[24]#011train-auc:0.68041#011validation-auc:0.59318\n",
      "[25]#011train-auc:0.68091#011validation-auc:0.59230\n",
      "[26]#011train-auc:0.68143#011validation-auc:0.59407\n",
      "[27]#011train-auc:0.68201#011validation-auc:0.59416\n",
      "[28]#011train-auc:0.68241#011validation-auc:0.59445\n",
      "[29]#011train-auc:0.68308#011validation-auc:0.59447\n",
      "[30]#011train-auc:0.68416#011validation-auc:0.59444\n",
      "[31]#011train-auc:0.68508#011validation-auc:0.59391\n",
      "[32]#011train-auc:0.68593#011validation-auc:0.59375\n",
      "[33]#011train-auc:0.68659#011validation-auc:0.59310\n",
      "[34]#011train-auc:0.68713#011validation-auc:0.59270\n",
      "[35]#011train-auc:0.68794#011validation-auc:0.59201\n",
      "[36]#011train-auc:0.68866#011validation-auc:0.59200\n",
      "[37]#011train-auc:0.68957#011validation-auc:0.59243\n",
      "[38]#011train-auc:0.69024#011validation-auc:0.59188\n",
      "[39]#011train-auc:0.69053#011validation-auc:0.59139\n",
      "[40]#011train-auc:0.69135#011validation-auc:0.59215\n",
      "[41]#011train-auc:0.69171#011validation-auc:0.59252\n",
      "[42]#011train-auc:0.69212#011validation-auc:0.59236\n",
      "[43]#011train-auc:0.69245#011validation-auc:0.59203\n",
      "[44]#011train-auc:0.69293#011validation-auc:0.59217\n",
      "[45]#011train-auc:0.69325#011validation-auc:0.59116\n",
      "[46]#011train-auc:0.69384#011validation-auc:0.59044\n",
      "[47]#011train-auc:0.69415#011validation-auc:0.59052\n",
      "[48]#011train-auc:0.69451#011validation-auc:0.59036\n",
      "[49]#011train-auc:0.69493#011validation-auc:0.59141\n",
      "[50]#011train-auc:0.69548#011validation-auc:0.59189\n",
      "[51]#011train-auc:0.69597#011validation-auc:0.59102\n",
      "[52]#011train-auc:0.69669#011validation-auc:0.59065\n",
      "\n",
      "2026-02-02 06:23:43 Uploading - Uploading generated training model\n",
      "2026-02-02 06:23:51 Completed - Training job completed\n",
      "Training seconds: 446\n",
      "Billable seconds: 446\n",
      "\n",
      "Training complete!\n",
      "Model artifacts: s3://sagemaker-us-east-1-786869526001/aai540-group1/models/engineered-no-target-encoding/sagemaker-xgboost-2026-02-02-06-15-34-211/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting training job...\")\n",
    "\n",
    "# Train the model\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "}, wait=True)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Model artifacts: {xgb_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3d49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model to Model Registry...\n",
      "Model artifacts: s3://sagemaker-us-east-1-786869526001/aai540-group1/models/engineered-no-target-encoding/sagemaker-xgboost-2026-02-02-06-15-34-211/output/model.tar.gz\n",
      "Container image: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.5-1\n",
      "Model metadata: {'model_type': 'engineered-no-target-encoding', 'num_features': '16', 'training_samples': '4299046', 'validation_samples': '482878'}\n",
      "\n",
      "✓ Model registered successfully!\n",
      "Model Package ARN: arn:aws:sagemaker:us-east-1:786869526001:model-package/flight-delay-models/2\n",
      "Model Package Group: flight-delay-models\n",
      "Approval Status: PendingManualApproval\n",
      "\n",
      "Metadata:\n",
      "  Model Type: engineered-no-target-encoding\n",
      "  Features: ['MONTH', 'DAY', 'DAY_OF_WEEK', 'DEP_HOUR', 'SCHEDULED_DEPARTURE', 'HOUR_SIN', 'HOUR_COS', 'IS_PEAK_HOUR', 'IS_WEEKEND', 'DISTANCE', 'SCHEDULED_TIME', 'IS_LONG_HAUL', 'DISTANCE_BUCKET', 'ORIGIN_FLIGHTS', 'DEST_FLIGHTS', 'ROUTE_FLIGHTS']\n",
      "  Training Samples: 4,299,046\n",
      "  Validation Samples: 482,878\n"
     ]
    }
   ],
   "source": [
    "# Register model to SageMaker Model Registry using boto3 client\n",
    "\n",
    "print(\"Registering model to Model Registry...\")\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "model_package_group_name = \"flight-delay-models\"\n",
    "\n",
    "# Get model artifacts path from estimator\n",
    "try:\n",
    "    model_data = xgb_estimator.model_data\n",
    "except:\n",
    "    # If estimator state lost, find the latest model from S3\n",
    "    print(\"Fetching latest model artifact from S3...\")\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=PERSONAL_BUCKET,\n",
    "        Prefix='aai540-group1/models/engineered-no-target-encoding/'\n",
    "    )\n",
    "    folders = set()\n",
    "    for obj in response.get('Contents', []):\n",
    "        parts = obj['Key'].split('/')\n",
    "        if len(parts) >= 4:\n",
    "            folders.add(parts[3])\n",
    "    latest_job = sorted(folders)[-1]\n",
    "    model_data = f\"s3://{PERSONAL_BUCKET}/aai540-group1/models/engineered-no-target-encoding/{latest_job}/output/model.tar.gz\"\n",
    "\n",
    "print(f\"Model artifacts: {model_data}\")\n",
    "print(f\"Container image: {container}\")\n",
    "\n",
    "# Create inference specification\n",
    "inference_spec = {\n",
    "    \"Containers\": [\n",
    "        {\n",
    "            \"Image\": container,\n",
    "            \"ModelDataUrl\": model_data\n",
    "        }\n",
    "    ],\n",
    "    \"SupportedContentTypes\": [\"text/csv\"],\n",
    "    \"SupportedResponseMIMETypes\": [\"text/csv\"],\n",
    "    \"SupportedRealtimeInferenceInstanceTypes\": [\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    \"SupportedTransformInstanceTypes\": [\"ml.m5.large\"]\n",
    "}\n",
    "\n",
    "# Build CustomerMetadataProperties\n",
    "customer_metadata = {\n",
    "    \"model_type\": \"engineered-no-target-encoding\",\n",
    "    \"num_features\": str(len(ENGINEERED_FEATURES)),\n",
    "    \"training_samples\": str(len(train_df)),\n",
    "    \"validation_samples\": str(len(val_df))\n",
    "}\n",
    "\n",
    "print(f\"Model metadata: {customer_metadata}\")\n",
    "\n",
    "# Create model package\n",
    "response = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelPackageDescription=f\"Engineered baseline - {len(ENGINEERED_FEATURES)} features excluding target-encoded rates. Features: MONTH DAY DAY_OF_WEEK DEP_HOUR SCHEDULED_DEPARTURE HOUR_SIN HOUR_COS IS_PEAK_HOUR IS_WEEKEND DISTANCE SCHEDULED_TIME IS_LONG_HAUL DISTANCE_BUCKET ORIGIN_FLIGHTS DEST_FLIGHTS ROUTE_FLIGHTS\",\n",
    "    InferenceSpecification=inference_spec,\n",
    "    ModelApprovalStatus='PendingManualApproval',\n",
    "    CustomerMetadataProperties=customer_metadata\n",
    ")\n",
    "\n",
    "model_package_arn = response['ModelPackageArn']\n",
    "\n",
    "print(f\"\\n✓ Model registered successfully!\")\n",
    "print(f\"Model Package ARN: {model_package_arn}\")\n",
    "print(f\"Model Package Group: {model_package_group_name}\")\n",
    "print(f\"Approval Status: PendingManualApproval\")\n",
    "print(f\"\\nMetadata:\")\n",
    "print(f\"  Model Type: engineered-no-target-encoding\")\n",
    "print(f\"  Features: {ENGINEERED_FEATURES}\")\n",
    "print(f\"  Training Samples: {len(train_df):,}\")\n",
    "print(f\"  Validation Samples: {len(val_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cb571",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Model Details\n",
    "- **Model Type**: Engineered Baseline (No Target Encoding)\n",
    "- **Features Used**: 16 engineered features\n",
    "  - Temporal (9): `MONTH`, `DAY`, `DAY_OF_WEEK`, `DEP_HOUR`, `SCHEDULED_DEPARTURE`, `HOUR_SIN`, `HOUR_COS`, `IS_PEAK_HOUR`, `IS_WEEKEND`\n",
    "  - Distance (4): `DISTANCE`, `SCHEDULED_TIME`, `IS_LONG_HAUL`, `DISTANCE_BUCKET`\n",
    "  - Volume (3): `ORIGIN_FLIGHTS`, `DEST_FLIGHTS`, `ROUTE_FLIGHTS`\n",
    "- **Algorithm**: XGBoost (binary:logistic)\n",
    "- **Training Instance**: ml.m5.xlarge\n",
    "- **Registry Status**: Registered to `flight-delay-models` group (PendingManualApproval)\n",
    "\n",
    "### Hyperparameter Changes from Raw Baseline\n",
    "- **Learning rate (eta)**: 0.05 (was 0.1) - slower, more careful learning\n",
    "- **Iterations (num_round)**: 1000 (was 500) - allow longer training\n",
    "- **Early stopping**: 50 rounds (was 30) - more patience before stopping\n",
    "\n",
    "### Comparison Notes\n",
    "- **vs Raw Baseline (6 features)**: Adds 10 engineered features for improved temporal and volume signals\n",
    "- **vs Full Engineered (20 features)**: Excludes 4 target-encoded delay rates to avoid potential leakage\n",
    "- Model artifacts: `s3://sagemaker-us-east-1-786869526001/aai540-group1/models/engineered-no-target-encoding/`\n",
    "- Training logs contain validation AUC metrics from early stopping rounds\n",
    "\n",
    "### Next Steps\n",
    "1. Evaluate on test set and compare metrics with raw baseline\n",
    "2. Optionally train full 20-feature model with target encoding for comparison\n",
    "3. Approve best model in registry for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
